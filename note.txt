export PYTHONPATH=/home/lpr/project_nattabude/carla/self-driving-sb3:$PYTHONPATH
export PYTHONPATH=$PWD:$PYTHONPATH
git reset --soft HEAD~1
git push --force



/home/lpr/TensorRT-8.6.1.6/targets/x86_64-linux-gnu/lib
python3 -m pip install tensorrt-*-cp38-none-linux_x86_64.whl
python3 -m pip install onnx_graphsurgeon-0.3.12-py2.py3-none-any.whl

export PATH=/usr/local/cuda-12.4/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}:/home/lpr/TensorRT-8.6.1.6/targets/x86_64-linux-gnu/lib


# how to find cuda path
find $HOME -name nvcc 2>/dev/null
find /usr/local -name nvcc 2>/dev/null


python train.py TQC TQC1 --level 0 --total_timesteps 50000 
python train.py TQC TQC1 continuous --level 1 --load_model RLmodel/TQC_3/model_200000_steps.zip
python train.py TQC TQC1 continuous --level 2 --load_model RLmodel/TQC_4/model_300000_steps.zip

positional arguments:
  algorithm             The algorithm to use
  algo_config           The model configuration

optional arguments:
  -h, --help            show this help message and exit
  --obs_module {observer_con_manv,observer_discrete_manv,observer_raw}
                        name of observer available in config/observer_config (default:
                        "observer_con_manv")
  --act_wrapper {action_limit,action_original}
                        name of observer available in config/action_config (default:
                        "action_limit")
  --discrete_actions {discrete_actions1}
                        name of observer available in config/action_config (default: None)
  --map_name MAP_NAME   The name of the map (default: AIT)
  --level LEVEL         The level (default: 0)
  --load_model LOAD_MODEL
                        The path to the model to load (default: "")
  --total_timesteps TOTAL_TIMESTEPS
                        total_timesteps
  --render              render image while training (default: True)
  --log_dir LOG_DIR     train log directory (default: "runs/RL")
  --save_dir SAVE_DIR   model save directory (default: "RLmodel")



python train.py PPO PPO1 continuous --level 0

python test_env.py --mode auto --level 0
python test_env.py --mode manual --level 2

python eval.py RLmodel/TQC_5/model_1300000_steps.zip --level 2 --eps 5 --seed 1234 --record

RLmodel/SAC_19/model_50000_steps.zip

TQC_6 -> reward sigmoid velo *4

RLmodel/SAC_34/model_1600000_steps.zip

RLmodel/SAC_41/model_700000_steps.zip

RLmodel/SAC_42/model_1500000_steps.zip